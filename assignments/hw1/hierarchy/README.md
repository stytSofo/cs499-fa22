Answer part 2 here.

All the tests were made on CloudLab's r650 machines since at the time of the experiment they were the only available machines. An r650 machine has 2 Intel Xeon 8360Y CPUs, 256GB of DDR4 3200 MT/s RAM, 1.2 TB of NVMe storage and a 25Gbit ethernet NIC. 

<!-- An interesting observation from the specs of the r650 is that the RAM is configured to run in tripple channel configuration which in turn will result in higher memory bandwitdth as we will see later.  -->

In terms of DRAM memory, the system has 2 NUMA nodes each with 128GB of ram. When the CPU is accessing its own NUMA node the latency is arounf 90ns compared to the other's CPU NUMA node where the latency is increased at around 150ns. The latency is about 1.7 times slower when accessing the other NUMA node. As for the bandwidth, when the CPU accesses its own NUMA node, the recorded bandwidths is around 170GB/s. When compared to the other NUMA Node the bandwidth is only about 55GB/s. This is a huge difference and in memory intensive applications a conciderable amount of loss in performance is expected. When measuring the remote node's RAM latency, a big overhead is added due to the latency of the network at around 0.165ms which is 10000 times slower than RAM. Since the overhead is so big, there is no real world differenece in latency between each CPU's NUMA node. Same goes for the bandwidth; It is limited by the bandwidth of the network at around 1.3GB/s.


Accessing the local node's disk reveals latency of 0.092ms. This is an impressive number considering the typical latency found in most normal hard drives. Such number is achived due to the machine's NMVe SSD. This particular drive is amongst the fastest available thus offering such impressive numbers. In IOPS heavy appilications this SSD would perform very well. In terms of bandwidth, this drive achieves speeds of up to 1.5GB/s in both read and write scenarios. Heavy bandwidth applications would bennefit form this drive's sequential performance. This is also due to the connection interface of the NVMe, utilizing the latest PCIe version 4 which in this case is directly connected to the CPU's controller rather than the motherboard's chipset. When accessing the remote drive, there is a similar increase in delay due to network's overhead. Bandwidth does not have a huge impact since the network bandwidth is close to the disk's speed. This means that accessing a remote disk does not really impact the overall performance when the workload focuses on sequential reads and writes. An interesting observation to be made is that when the hardware is very fast, the only overhead added to remote communication is the network's bandwidth and latency. Moreover, we can see that in terms of bandwidth both the remote RAM and remote Disk have the same sequential speeds. The local disk's latency is about the half of the remote's RAM which is interesting since it's not as dramatic of a difference as the latency between the local disk's latency and loca ram's latency.


Finally, capacity is pretty much the same across the local and remote nodes since they are the exact same hardware. Both machines are fitted with 256GB of DDR4 3200MT/s ram (16 dimms 16GB each). Each CPU has a NUMA node with 128GB in an eight-channel configuration meaning thus offering 170GB/s sequential read/write speeds. 